# 大模型能学习规则

LARGE LANGUAGE MODELS CAN LEARN RULES

## 摘要

由于大模型在基于隐式知识回答问题时常常会出现幻觉，我们采用 Hypothesis-to-Theories （HtT）的方法，让大模型学习规则库来进行推理。

Hypotheses-to-Theories包含两个阶段，它们分别是归纳阶段（induction stage）和演绎阶段（deduction stage）。

* 归纳阶段：首先请求LLM在训练样本上生成并验证规则，以此形成规则库
* 演绎阶段：利用规则库中学会的规则进行推理并回答测试问题

实验表明，在数学推理和关系推理问题上，精确度提升了11%-27%。学习到的规则也可以迁移到不同的模型和同一问题的不同形式上。

## 引言

**背景**：随着LLM规模的增长，它们在推理任务上的能力也更加强大。通过高级的提示技术，例如让LLM将复杂问题分解成简单的小问题并一步步解决，能够让LLM的能力得到进一步激发。

**问题**：
* LLM会生成看似合理但与真实世界知识相矛盾的输出，即幻觉问题。
* 当任务偏离所需的常规知识时（如非十进制运算），LLM推理的准确率显著下降。

这两个问题产生的原因是**预训练大模型中隐式知识和任务所要求知识的不匹配**。
虽然可以通过有监督微调编码数据集并向LLM注入所需知识，但在推理问题中**自动发现和应用知识**的通用方法将是更加可取的。

为了解决这个问题，我们重新考虑了人类在建立关于世界理论中科学发现的过程。
通常，一个科学发现来自于科学家的假设，并通过实验验证假设。这些理论能够被应用在新的场景以解决问题。有意思的是，即使最初的假设不完全正确，这个过程最后也能够获得正确的知识。（例如地心说和日心说）。也就是说，这个过程一开始是允许人类自由地提出假设的，但理论只有被实验验证了才能够为真。

为此，我们提出了假设到理论的框架，包含一个归纳阶段(induction stage)和一个演绎阶段(deduction stage)。在归纳阶段，使用CoT来声明规则并推导出答案。规则将根据其频率和准确性，被收集形成规则库。在演绎阶段，在CoT提示之前添加规则库，并要求LLM检索规则以推导出答案。这将隐式推理问题(implicit reasoning problem)转换为显式推理问题(explicit reasoning problem)。

为了减少prompt工程所需的工作量，我们提出了从演绎中归纳的方法，将规则生成和验证步骤融合到一个类似演绎的步骤中。这样，两个阶段的提示都可以很容易地从现有的少量提示方法中获得，如思维链提示（chain-of-thought）或最少到最多提示（least-to-most prompting）。

我们在Arithmetic（2023）和 CLUTRR（2019）两个数据集上分别用GPT3.5和GPT4验证了有效性。

## 心得

论文的方法包含一个归纳阶段( induction stage)和一个演绎阶段(deduction stage)，似于传统机器学习中的训练和测试。

* 在归纳阶段，使用CoT来声明规则并推导出答案。规则将根据其频率和准确性，被收集形成规则库。
* 在演绎阶段，在CoT提示之前添加规则库，并要求LLM检索规则以推导出答案。这将隐式推理问题(implicit reasoning problem)转换为显式推理问题(explicit reasoning problem)。
