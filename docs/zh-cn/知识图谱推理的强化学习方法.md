# 知识图谱推理的强化学习方法

DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning

## 摘要

* 问题：在大规模知识图谱上的推理
* 方法：提出强化学习框架以解决多跳关系路径问题
  * 基于知识图谱嵌入，在连续状态空间中，采用Policy-Based智能体推理
  * 通过在其路径上采样最有可能的关系来拓展延申路径
* 创新点：设计的奖励函数中同时考虑了精度、多样性和效率
* 结果：在Freebase和NELL数据集上比基于路径排序的算法和知识图谱嵌入的方法更好

## 引言

### 问题介绍

自然语言处理问题往往需要多个**相互关联的决策**（inter-related decisions），赋予深度学习模型学习推理能力仍然具有挑战。

要处理无明显答案的复杂查询（queries），智能机器必须能够利用现有资源和信息进行推理，学会推断未知的答案。

本文研究的问题为**多跳推理**（multi-hop reasoning），即给定一个大的知识图谱，然后来学习显式推理公式。

例如，如果KG包含一个知识：内马尔效力于巴塞罗那，并且巴塞罗那在西甲联赛中。那么机器就应该能够来学习下列关系： 

$$playerPlaysForTeam(P,T) \land teamPlaysInLeague(T,L) \\ \Rightarrow playerPlaysInLeague(P,L)$$

在测试时间内，通过所学到的公式，系统应该能够自动推断出一对实体之间缺失的链接关系。这种推理机有可能成为复杂QA系统的重要组成部分。

### 路径排序算法（PRA）

近年来提出的路径排序算法（Path-Ranking Algorithm，PRA）是大型知识图谱中学习推理路径的一种有效方法。

路径排序算法使用的是随机游走算法，通过基于重启动推理机制的随机游走（random-walk with restarts based inference mechanism）来实现多个有界的深度优先搜素过程，在搜索中查找关系路径。再加上基于弹性网（elastic net）的学习，PRA使用监督学习选择更合理的路径。

然而，PRA是在一个完全离散的空间中运行的，这使得在一个KG中评估和比较相似的实体和关系变得困难。

## 补充资料

### 路径排序算法

Relational retrieval using a combination of path-constrained random walks

文章提出的PRA算法是知识图谱推理的早期探索，在RWR（重启随机游走算法）的基础上进行了相似性的改进。同时在那个机器学习还没有普及的年代，文章也探索了使用监督学习的方法进行参数的学习和训练。

#### 任务

文章提出了四个任务以评估提出的PRA算法的有效性，虽然是针对生物医学领域的，但其实也都是和推荐系统相关的任务：

1. 期刊推荐：输入：论文标题中的专业术语，与文章相关的关键字（基因或蛋白质），现在的年份。输出：推荐的期刊及其排名。该任务有助于预印本论文发表。
2. 引文推荐：输入与期刊推荐的输入相同。输出：推荐的论文及其排名。该任务有助于预印本论文发表。
3. 专家发现：输入与期刊推荐的输入相同。输出推荐的专家及其排名。该任务有助于发现合适的审稿人或者新的合作者。
4. 基因推荐：输入作者以及年份，输出推荐的基因及其排名。这项任务类似于预测该作者未来的研究兴趣。

#### 方法

文章其实是希望能够运用图上的游走方法来达到推荐的效果。例如，对于期刊推荐任务，任务有许多的本体起始点（Title Word，gene，protein，Year等）。然后通过在知识图谱上的游走最终停留在类型为"journal"的实体上，停留概率最大的即为推荐的期刊。

#### 算法

传统的重启随机游走算法为每一个类型的边设置了各自的转移概率，但作者认为这种方法忽略了上下文的影响，作者举了个例子：
在引文推荐任务中，假定以“year” y 为起点寻找推荐的引文，可能会得到以下两种情况的推荐：
1）查找在y年发表的论文
2）查找y年发表的论文经常引用的论文

第一种情况推荐的是，$year \rightarrow PublishedIn^{-1} \rightarrow paper$
第二种情况推荐的是，$year \rightarrow PublishedIn^{-1} \rightarrow  paper \rightarrow Cite \rightarrow paper$

直觉上来说第二种情况得到的推荐比第一种情况得到的推荐更合适。也就是说对于推荐而言，可能某种路径下得到的推荐是更为合适的？因此应该为不同的路径设置不同的转移概率？


