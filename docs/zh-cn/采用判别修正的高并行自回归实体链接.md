# 采用判别修正的高并行自回归实体链接

## 摘要

过往的实体链接自回归生成方法存在以下缺点：

1. 计算成本高昂（依赖复杂且深的Transformer解码器）
2. 非并行化解码（自回归解码器，排除了across mention的并行）
3. 需要大规模数据训练

本文提出了将所有潜在的mention并行进行实体链接的方法，并依赖一个浅层且高效的解码器。

此外，在生成目标的基础上增加了一个额外的判别成分，即一个修正项，使其可以直接优化生成器的排序。

本文的模型解决了上述的缺点，并且比之前的生成模型快70倍以上且更加精确。在AIDA-CoNLL数据集上达到了state-of-the-art。

## 介绍

采用自回归语言模型可以更好地利用预训练过程中积累的隐含知识，充分利用实体及其上下文的全交叉编码器（full cross-encoder）

* 对于Entity Disambiguation：自回归生成表现得非常好
* 对于Entity Linking：虽然在许多数据集上达到了SOTA，但受到许多限制

