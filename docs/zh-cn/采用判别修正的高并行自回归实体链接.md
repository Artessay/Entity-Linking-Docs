# 采用判别修正的高并行自回归实体链接

## 摘要

过往的实体链接自回归生成方法存在以下缺点：

1. 计算成本高昂（依赖复杂且深的Transformer解码器；Transformer无状态，随着序列变长内存空间增大）
2. 非并行化解码（自回归解码器，排除了across mention的并行）
3. 需要大规模数据训练

本文提出了将所有潜在的mention并行进行实体链接的方法，并依赖一个浅层且高效的解码器。

此外，在生成目标的基础上增加了一个额外的判别成分，即一个修正项，使其可以直接优化生成器的排序。

本文的模型解决了上述的缺点，并且比之前的生成模型快70倍以上且更加精确。在AIDA-CoNLL数据集上达到了state-of-the-art。

## 介绍

采用自回归语言模型可以更好地利用预训练过程中积累的隐含知识，充分利用实体及其上下文的全交叉编码器（full cross-encoder）

* 对于Entity Disambiguation：自回归生成表现得非常好
* 对于Entity Linking：虽然在许多数据集上达到了SOTA，但受到许多限制

本文重新审视了实体链接的**生成方法**，在给定输入的情况下，有条件地独立生成mention-entity对。

* 此方法通过使用一个浅层的LSTM decoder允许跨mention的并行。
* 为了更明确地优化生成器的排名，我们使用了一个判别性的修正项，该修正项推动正确预测的分数高于其余的分数。
* 为了支持长文本输入，采用了高效的Transformer encoder来支持长序列。

## 模型概览

![Alt text](_img/parallel-fig1.png)

一个基于Transformer的document encoder将文档嵌入向量中。然后mention detection模块识别文档中的mention span。在提及嵌入的条件下，实体链接模块首先使用LSTM来生成或为候选的文本标识符评分，然后使用分类器对所有候选者进行重新排序。

## 背景

* 实体链接
  * 在输入文本中找寻mention-entity对
* 相关工作
  * 大多数方法将MD和ED分开处理
  * 近来，端到端实体链接采用共享架构
  * 本文聚焦英语，有工作探索来跨语言实体链接
* 自回归链接
  * 不再将EL作为向量空间中的匹配问题，而将其作为sequence-to-sequence的序列到序列问题
    * 匹配mention向量到entity embedding：maximum inner-product search（MIPS）
    * 生成模型：
      * 利用预训练只是
      * 不需要存储预计算的实体表示，节省内存
      * 在上下文和实体上完全跨编码器
