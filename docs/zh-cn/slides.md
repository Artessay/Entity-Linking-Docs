---
marp: true
---

# 大模型时代实体链接的危与机

——实体链接论文阅读分享

2023.09.04

---

### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

现有方法的挑战
* 知识库增长迅速，非零次学习模型无法链接在训练集中没有出现过的实体
* 现有能够实现零次学习的实体链接模型推理速度比非零次学习的模型慢一个数量级

作者的假设
* 实体的分类信息对于实体链接是有帮助的
* 实体的描述信息对于实体链接是有帮助的
* 实体的分类信息和描述信息不依赖模型，易于扩展
* 综合细粒度的实体分类和实体描述信息，能够有效实现实体链接

---


### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

提及检测
* 采用BIO标注格式编码提及
* 利用一个线性层来进行词分类（B、I、O），用交叉熵作为损失训练
* 对每个提及，通过平均池化上下文的词嵌入获得语境信息

实体消歧
* 实体分类分数
  * 计算每一个提及对于各个类型的分类情况向量
  * 计算提及类型和实体类型的欧拉距离作为实体分类分数
* 实体描述分数
  * 采用一个双编码器编码将文档中的所有提及编码
  * 采用另一个Transformer模型编码实体及其描述
  * 计算两个编码向量的内积作为实体描述分数
* 实体优先分数
  * 特定提及下对应实体的条件概率


---


### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

* 为什么能实现零次学习？
  * 针对实体分类和描述进行打分，实体分类器和描述嵌入模型训练好之后可以对未见过的实体生成分类和描述，从而实现零样本学习
  * 采用Transformer模型完成提及检测、实体分类和实体消歧，仅需一次扫描，速度快，且能够进一步finetune
* 不足
  * 模型实现了新的实体加入时不需要重新训练模型，但是原有模型在训练过程中依然需要有标记的数据


---

### 采用判别修正的高并行自回归实体链接

现有方法的挑战
* 依赖Transformer架构的模型计算成本高昂
  * Transformer是无状态的，随着序列长度增长内存消耗巨大
  * Transformer训练需要大量数据
  * 采用同一个解码器，难以跨提及并行

