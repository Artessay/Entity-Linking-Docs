---
marp: true
---

# 大模型时代实体链接的危与机

——实体链接论文阅读分享

2023.09.04

---

### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

现有方法的挑战
* 知识库增长迅速，非零次学习模型无法链接在训练集中没有出现过的实体
* 现有能够实现零次学习的实体链接模型推理速度比非零次学习的模型慢一个数量级

作者的假设
* 实体的分类信息对于实体链接是有帮助的
* 实体的描述信息对于实体链接是有帮助的
* 实体的分类信息和描述信息不依赖模型，易于扩展
* 综合细粒度的实体分类和实体描述信息，能够有效实现实体链接

---


### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

提及检测
* 采用BIO标注格式编码提及
* 利用一个线性层来进行词分类（B、I、O），用交叉熵作为损失训练
* 对每个提及，通过平均池化上下文的词嵌入获得语境信息

实体消歧
* 实体分类分数
  * 计算每一个提及对于各个类型的分类情况向量
  * 计算提及类型和实体类型的欧拉距离作为实体分类分数
* 实体描述分数
  * 采用一个双编码器编码将文档中的所有提及编码
  * 采用另一个Transformer模型编码实体及其描述
  * 计算两个编码向量的内积作为实体描述分数
* 实体优先分数
  * 特定提及下对应实体的条件概率


---


### 一种有效的零样本端到端实体链接方法

——解决数据扩充痛点的零次学习方法

* 为什么能实现零次学习？
  * 针对实体分类和描述进行打分，实体分类器和描述嵌入模型训练好之后可以对未见过的实体生成分类和描述，从而实现零样本学习
  * 采用Transformer模型完成提及检测、实体分类和实体消歧，仅需一次扫描，速度快，且能够进一步finetune
* 不足
  * 模型实现了新的实体加入时不需要重新训练模型，但是原有模型在训练过程中依然需要有标记的数据


---

### 采用判别修正的高并行自回归实体链接

现有方法的挑战
* 依赖Transformer架构的模型计算成本高昂
  * Transformer是无状态的，随着序列长度增长内存消耗巨大
  * Transformer训练需要大量数据
  * 采用同一个解码器，难以跨提及并行

---

### 采用判别修正的高并行自回归实体链接

特点
* 针对长文本的高效实体链接而生

方法
* 词编码
  * 采用支持长序列编码的Longformer
* 提及检测
  * 将提及短语(ms, me)出现的概率分解为以ms开始的概率乘以在ms开始的前提下以me结尾的概率
  * 将提及的长度限制在15词之内，保证长文本识别效率
  * 在推理时只考虑ms起始概率大于阈值的提及，缩小搜索空间
* 实体消歧
  * 从左到右逐词生成实体的唯一名词
  * 用集束搜索减少搜索空间和保证生成名词在知识库中
  * 采用小型单层LSTM，并利用一个额外的机遇判别式分类器的损失函数训练
* 参数估计
  * 采用随机梯度下降法，联合优化各个部件使得模型的似然概率最大


---

### 采用判别修正的高并行自回归实体链接

生成式实体链接的优点
* 提供提及和实体之间更多的交互
  * 采用标题而非ID表示实体，在提及和实体相似时能提供更多的上下文交互
* 节省内存
  * 只需要保存每个实体的前缀树，不需要保存实体向量
* 不需要负采样
  * 分类任务的候选词表太大，无法对所有候选实体排序，需要负采样帮助训练；但生成任务中的损失可以直接计算得到

思路借鉴
* 采用生成式方法，将分类任务转为生成任务完成

不足
* 依然需要先使用有标签的数据训练

---

# 多语言自回归实体链接

方法列举
* 规范化实体表示
  * 根据提及出现频率，在该实体的所有可用语言中选择**一个文本标识符**作为其名称
  * 缺点：不能有效利用上下文和实体名称之间的词汇重叠
* 多语言实体表示
  * 生成‘语言ID + 实体名称’的方式，同时生成多语言的实体表述
  * 优点：可以利用源语言和目标语言之间的协同作用，并避免翻译
* 边缘化
  * 将文本视为潜在变量，计算实体名称在所有语言中出现的概率之和
  * 缺点：计算代价相对较大
* 候选实体选择
  * 采用集束搜索选择有效实体候选者进行评分

---

# 多语言自回归实体链接

结果分析
* title+lang的效果是较好的，配合候选实体选择（cand）和边缘化（marg）有着不同的效果

---

# 多语言自回归实体链接

优势
* 将所有可用语言作为目标，能够处理没有训练数据的源语言的零样本学习
* 利用不同语言提及表述和实体名称之间的联系，提高模型准确性
  * 同语系下同一实体的表述通常是相近的，比如英文的potato和西班牙语的patata

